"""
Key Balancer with LRU caching and weight-based selection using SSOT pattern.
All data is stored in and retrieved from SQLite database.
"""

import random
import time
import functools
from typing import List, Optional, Tuple, Callable, Any, Dict
from collections import OrderedDict
from datetime import datetime

from .key_manager import KeyManager, APIKey


class LRUCache:
    """Simple LRU cache implementation optimized for large key sets."""
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.access_count = 0
    
    def get(self, key: str) -> Optional[APIKey]:
        """Get an item from cache and mark it as recently used."""
        if key in self.cache:
            # Move to end (most recently used)
            self.cache.move_to_end(key)
            self.access_count += 1
            return self.cache[key]
        return None
    
    def put(self, key: str, value: APIKey):
        """Put an item in cache."""
        if key in self.cache:
            # Move to end (most recently used)
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.capacity:
                # Remove least recently used item
                self.cache.popitem(last=False)
        self.cache[key] = value
    
    def clear(self):
        """Clear the cache."""
        self.cache.clear()
        self.access_count = 0
    
    def get_stats(self) -> dict:
        """Get cache statistics."""
        return {
            'size': len(self.cache),
            'capacity': self.capacity,
            'hit_rate': self.access_count / max(len(self.cache), 1),
            'access_count': self.access_count
        }


class KeyContext:
    """Context manager for automatic key health management."""
    
    def __init__(self, balancer, keys: List[str]):
        self.balancer = balancer
        self.keys = keys
        self.success = False
    
    def __enter__(self):
        return self.keys
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is None:
            # æ²¡æœ‰å¼‚å¸¸ï¼Œè‡ªåŠ¨æ ‡è®°ä¸ºæˆåŠŸ
            self.success = True
            for key in self.keys:
                self.balancer._mark_key_success(key)
        else:
            # æœ‰å¼‚å¸¸ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤„ç†å¤±è´¥æƒ…å†µ
            # æ³¨æ„ï¼šè¿™é‡Œä¸è‡ªåŠ¨æ ‡è®°å¤±è´¥ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“é”™è¯¯ç å¤„ç†
            pass


class KeyBalancer:
    """
    Main key balancer that provides LRU caching and weight-based key selection.
    Optimized for large key sets (1000+ keys) with SQLite persistence using SSOT pattern.
    
    Now supports three improvement schemes:
    1. Auto-success mode: automatically mark keys as successful when retrieved
    2. Context manager: automatic key health management with context
    3. Decorator pattern: automatic key health management with decorators
    """
    
    def __init__(self, cache_size: int = 100, db_path: Optional[str] = None, 
                 auto_save: bool = True, auto_success: bool = True):
        """
        Initialize the key balancer.
        
        Args:
            cache_size: Size of the LRU cache for recently used keys
            db_path: Path to the SQLite database (defaults to XDG_DATA_HOME)
            auto_save: Whether to automatically save state periodically
            auto_success: Whether to automatically mark keys as successful when retrieved
        """
        # æ ¹æ®é¢„æœŸkeyæ•°é‡è‡ªåŠ¨è°ƒæ•´ç¼“å­˜å¤§å°
        if cache_size < 100:
            cache_size = max(100, cache_size)
        
        self.key_manager = KeyManager(
            db_path=db_path,
            auto_save=auto_save
        )
        self.lru_cache = LRUCache(cache_size)
        self.last_selection_time = 0
        self.min_selection_interval = 0.05  # é™ä½æœ€å°é€‰æ‹©é—´éš”ä»¥æ”¯æŒå¤§é‡key
        self.selection_count = 0
        self.auto_success = auto_success
        
        # åˆå§‹åŒ–æƒé‡åˆ†å¸ƒç›¸å…³å±æ€§
        self._cumulative_weights = []
        self._available_keys_list = []
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—æƒé‡åˆ†å¸ƒ
        self._update_weight_distribution()
    
    def _mark_key_success(self, key_value: str):
        """Internal method: mark key as successful."""
        key_obj = self.key_manager.get_key_by_value(key_value)
        if key_obj:
            key_obj.last_used = datetime.now()
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–æˆåŠŸé€»è¾‘ï¼Œæ¯”å¦‚å¢åŠ æƒé‡ç­‰
    
    def _update_weight_distribution(self):
        """Update the weight distribution with proper LRU selection."""
        available_keys = self.key_manager.get_available_keys()
        if not available_keys:
            self._available_keys_list = []
            self._cumulative_weights = []
            return
        
        # æŒ‰ LRU åŸåˆ™æ’åºï¼šlast_used ä¸º None çš„æ’åœ¨å‰é¢ï¼ˆä»æœªä½¿ç”¨è¿‡ï¼‰ï¼Œç„¶åæŒ‰ last_used å‡åº
        lru_sorted_keys = sorted(available_keys, key=lambda k: (k.last_used is None, k.last_used or datetime.min))
        
        # è¿‡æ»¤æ‰æœ€è¿‘æœ‰ 429 é”™è¯¯çš„ keysï¼ˆå†·å´æœŸï¼‰
        current_time = datetime.now()
        filtered_keys = []
        
        for key in lru_sorted_keys:
            # å¦‚æœæœ€è¿‘æœ‰ 429 é”™è¯¯ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
            if key.last_error and key.weight <= 0.2:  # æƒé‡å¾ˆä½è¯´æ˜å¯èƒ½æœ‰ 429 é”™è¯¯
                time_since_error = current_time - key.last_error
                if time_since_error.total_seconds() < 300:  # 5åˆ†é’Ÿå†·å´æœŸ
                    continue  # è·³è¿‡è¿™ä¸ª key
            filtered_keys.append(key)
        
        # å¦‚æœæ²¡æœ‰è¿‡æ»¤åçš„ keysï¼Œä½¿ç”¨åŸå§‹åˆ—è¡¨
        if not filtered_keys:
            filtered_keys = lru_sorted_keys
        
        # åªé€‰æ‹©å‰ 5 ä¸ªæœ€å°‘ä½¿ç”¨çš„ keysï¼Œé¿å…æ€»æ˜¯é€‰æ‹©ç›¸åŒçš„ key
        max_keys_to_consider = min(5, len(filtered_keys))
        selected_keys = filtered_keys[:max_keys_to_consider]
        
        # æ›´æ–°æƒé‡åˆ†å¸ƒ
        self._available_keys_list = selected_keys
        self._cumulative_weights = []
        
        if selected_keys:
            # è®¡ç®—æ€»æƒé‡
            total_weight = sum(key.weight for key in selected_keys)
            if total_weight > 0:
                cumulative = 0
                for key in selected_keys:
                    cumulative += key.weight
                    self._cumulative_weights.append((cumulative, key))
    
    def _calculate_time_decay_weight(self, last_used: Optional[datetime], current_time: float) -> float:
        """
        è®¡ç®—åŸºäºæ—¶é—´çš„æƒé‡è¡°å‡ã€‚
        
        æ—¶é—´è¡°å‡è§„åˆ™ï¼š
        - 10ç§’å†…ï¼šé™æƒ 0.5
        - 1åˆ†é’Ÿå†…ï¼šé™æƒ 0.1
        - ä½¿ç”¨æŒ‡æ•°ä¸‹é™ï¼šweight = base_weight * (0.5 ^ (time_diff / 10))
        
        Args:
            last_used: ä¸Šæ¬¡ä½¿ç”¨æ—¶é—´ï¼ˆdatetimeå¯¹è±¡ï¼‰
            current_time: å½“å‰æ—¶é—´æˆ³ï¼ˆfloatï¼‰
            
        Returns:
            æ—¶é—´è¡°å‡æƒé‡ï¼ˆ0.0 åˆ° 1.0 ä¹‹é—´ï¼‰
        """
        if last_used is None:
            return 1.0  # ä»æœªä½¿ç”¨è¿‡çš„keyä¿æŒåŸæƒé‡
        
        # å°†datetimeè½¬æ¢ä¸ºtimestamp
        last_used_timestamp = last_used.timestamp()
        time_diff = current_time - last_used_timestamp
        
        if time_diff <= 0:
            return 1.0  # åˆšåˆšä½¿ç”¨è¿‡ï¼Œä¿æŒåŸæƒé‡
        
        # æŒ‡æ•°ä¸‹é™ï¼šä»¥10ç§’ä¸ºåŸºå‡†å‘¨æœŸï¼Œ0.5ä¸ºè¡°å‡å› å­
        # weight = 0.5 ^ (time_diff / 10)
        decay_factor = 0.5
        time_period = 10.0  # 10ç§’
        
        time_decay_weight = decay_factor ** (time_diff / time_period)
        
        # ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…
        return max(0.01, min(1.0, time_decay_weight))
    
    def get_keys(self, count: int = 1) -> List[str]:
        """
        Get a specified number of available API keys using LRU and weight-based selection.
        Optimized for large key sets.
        
        Args:
            count: Number of keys to return
            
        Returns:
            List of API key strings
        """
        if count <= 0:
            return []
        
        # æ¯æ¬¡è·å–keysæ—¶éƒ½æ›´æ–°æƒé‡åˆ†å¸ƒï¼ˆåŒ…å«æ—¶é—´è¡°å‡ï¼‰
        self._update_weight_distribution()
        
        if not self._available_keys_list:
            raise RuntimeError("No available API keys")
        
        if count > len(self._available_keys_list):
            # If requesting more keys than available, return all available
            count = len(self._available_keys_list)
        
        # Apply rate limiting to prevent too frequent selections
        current_time = time.time()
        if current_time - self.last_selection_time < self.min_selection_interval:
            time.sleep(self.min_selection_interval - (current_time - self.last_selection_time))
        
        selected_keys = []
        
        # ä½¿ç”¨çœŸæ­£çš„ LRU é€‰æ‹©ï¼šé€‰æ‹©æœ€å°‘ä½¿ç”¨çš„ keys
        if count >= len(self._available_keys_list):
            selected_keys = self._available_keys_list.copy()
        else:
            # é€‰æ‹©å‰ count ä¸ªæœ€å°‘ä½¿ç”¨çš„ keys
            selected_keys = self._available_keys_list[:count]
        
        # Update LRU cache and mark keys as used
        for key in selected_keys:
            self.lru_cache.put(key.key, key)
            # ä»key_managerçš„keysåˆ—è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„keyå¯¹è±¡
            for original_key_obj in self.key_manager.keys:
                if original_key_obj.key == key.key:
                    original_key_obj.mark_used()
                    # æ‰“å° key ä½¿ç”¨ä¿¡æ¯
                    print(f"ğŸ”‘ ä½¿ç”¨ Key: {key.key[:20]}... | æƒé‡: {key.weight:.2f} | æ€»ä½¿ç”¨æ¬¡æ•°: {self.selection_count + 1}")
                    break
        
        self.last_selection_time = time.time()
        self.selection_count += 1
        
        # æ–¹æ¡ˆ1ï¼šè‡ªåŠ¨æˆåŠŸæ¨¡å¼
        key_strings = [key.key for key in selected_keys]
        if self.auto_success:
            for key_str in key_strings:
                self._mark_key_success(key_str)
        
        return key_strings
    
    def _fast_weighted_selection(self, keys: List[APIKey], count: int) -> List[APIKey]:
        """
        Fast weighted selection for large key sets using pre-computed cumulative weights.
        """
        if count >= len(keys):
            return keys.copy()
        
        # ä½¿ç”¨é¢„è®¡ç®—çš„æƒé‡åˆ†å¸ƒ
        self._update_weight_distribution()
        
        selected_keys = []
        available_keys = keys.copy()
        total_weight = sum(key.weight for key in available_keys)
        
        for _ in range(count):
            if not available_keys:
                break
            
            # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ä¼˜åŒ–æƒé‡é€‰æ‹©
            rand_val = random.uniform(0, total_weight)
            selected_key = self._binary_search_key(available_keys, rand_val)
            
            if selected_key:
                selected_keys.append(selected_key)
                available_keys.remove(selected_key)
                total_weight -= selected_key.weight
        
        return selected_keys
    
    def _binary_search_key(self, keys: List[APIKey], target_weight: float) -> Optional[APIKey]:
        """Binary search for key selection based on weight."""
        if not keys:
            return None
        
        left, right = 0, len(keys) - 1
        cumulative_weight = 0
        
        for i, key in enumerate(keys):
            cumulative_weight += key.weight
            if cumulative_weight >= target_weight:
                return key
        
        # Fallback to last key if not found
        return keys[-1] if keys else None
    
    def _standard_weighted_selection(self, keys: List[APIKey], count: int) -> List[APIKey]:
        """
        Standard weighted random selection for smaller key sets.
        """
        if count >= len(keys):
            return keys.copy()
        
        # Calculate total weight
        total_weight = sum(key.weight for key in keys)
        if total_weight <= 0:
            # Fallback to random selection if all weights are 0
            return random.sample(keys, count)
        
        selected_keys = []
        available_keys = keys.copy()
        
        for _ in range(count):
            if not available_keys:
                break
            
            # Weighted random selection
            rand_val = random.uniform(0, total_weight)
            cumulative_weight = 0
            
            for i, key in enumerate(available_keys):
                cumulative_weight += key.weight
                if cumulative_weight >= rand_val:
                    selected_keys.append(key)
                    available_keys.pop(i)
                    total_weight -= key.weight
                    break
        
        return selected_keys
    
    def get_single_key(self) -> str:
        """
        Get a single available API key.
        
        Returns:
            A single API key string
        """
        keys = self.get_keys(1)
        return keys[0] if keys else None
    
    def get_key_context(self, count: int = 1) -> KeyContext:
        """
        Get a context manager for automatic key health management.
        
        Args:
            count: Number of keys to return
            
        Returns:
            KeyContext object that can be used as a context manager
        """
        keys = self.get_keys(count)
        return KeyContext(self, keys)
    
    # æ–¹æ¡ˆ3ï¼šè£…é¥°å™¨æ¨¡å¼
    def with_key_balancing(self, key_count: int = 1, auto_success: bool = None):
        """
        Decorator for automatic key health management.
        
        Args:
            key_count: Number of keys to use
            auto_success: Whether to auto-mark as successful (defaults to instance setting)
            
        Returns:
            Decorator function
        """
        if auto_success is None:
            auto_success = self.auto_success
            
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # è·å– key
                keys = self.get_keys(key_count)
                
                try:
                    # æ‰§è¡Œ API è°ƒç”¨
                    result = func(*args, **kwargs)
                    # è‡ªåŠ¨æ ‡è®°ä¸ºæˆåŠŸ
                    if auto_success:
                        for key in keys:
                            self._mark_key_success(key)
                    return result
                except Exception as e:
                    # è‡ªåŠ¨æ ‡è®°ä¸ºå¤±è´¥ï¼ˆä½¿ç”¨é€šç”¨é”™è¯¯ç 500ï¼‰
                    for key in keys:
                        self.update_key_health(key, error_code=500)
                    raise
            return wrapper
        return decorator
    
    def update_key_health(self, key_value: str, error_code: Optional[int] = None, success: bool = False):
        """
        Update the health status of a key.
        
        Args:
            key_value: The actual key string
            error_code: HTTP error code if an error occurred
            success: Whether the request was successful
        """
        self.key_manager.update_key_health(key_value, error_code, success)
        
        # å¦‚æœkeyçŠ¶æ€å‘ç”Ÿé‡è¦å˜åŒ–ï¼Œæ›´æ–°æƒé‡åˆ†å¸ƒ
        if error_code == 400 or (success and error_code is None):
            self._update_weight_distribution()
    
    def get_stats(self) -> dict:
        """
        Get statistics about the key balancer and all keys.
        
        Returns:
            Dictionary containing statistics
        """
        stats = self.key_manager.get_key_stats()
        cache_stats = self.lru_cache.get_stats()
        
        # æ›´æ–°æƒé‡åˆ†å¸ƒä»¥è·å–å½“å‰çŠ¶æ€
        self._update_weight_distribution()
        
        stats.update({
            'cache_stats': cache_stats,
            'last_selection_time': self.last_selection_time,
            'selection_count': self.selection_count,
            'min_selection_interval': self.min_selection_interval,
            'auto_success_enabled': self.auto_success,
            'current_top_weight_keys': len(self._available_keys_list) if hasattr(self, '_available_keys_list') else 0,
            'weight_distribution_updated': hasattr(self, '_cumulative_weights') and len(self._cumulative_weights) > 0,
        })
        
        return stats
    
    def reload_keys(self):
        """Reload keys from database."""
        self.key_manager._load_from_database()
        self.lru_cache.clear()
        self._update_weight_distribution()
    
    def reset_all_weights(self):
        """Reset weights for all keys."""
        self.key_manager.reset_all_weights()
        self.lru_cache.clear()
        self._update_weight_distribution()
    
    def get_key_info(self, key_value: str) -> Optional[dict]:
        """
        Get detailed information about a specific key.
        
        Args:
            key_value: The actual key string
            
        Returns:
            Dictionary containing key information or None if not found
        """
        key = self.key_manager.get_key_by_value(key_value)
        if not key:
            return None
        
        return {
            'key': key.key[:8] + '...' if len(key.key) > 8 else key.key,
            'weight': round(key.weight, 2),
            'available': key.is_available,
            'error_count': key.error_count,
            'consecutive_errors': key.consecutive_errors,
            'last_used': key.last_used.isoformat() if key.last_used else None,
            'last_error': key.last_error.isoformat() if key.last_error else None,
            'in_cache': key.key in self.lru_cache.cache,
            'added_time': key.added_time.isoformat(),
            'source': key.source,
        }
    
    def save_state_now(self):
        """Manually save state immediately."""
        self.key_manager.save_state_now()
    
    def cleanup_old_keys(self, days_old: int = 30):
        """Remove keys that haven't been used for specified days."""
        removed_count = self.key_manager.cleanup_old_keys(days_old)
        if removed_count > 0:
            self._update_weight_distribution()
        return removed_count
    
    def get_memory_usage(self) -> dict:
        """Get memory usage statistics."""
        return self.key_manager.get_memory_usage()
    
    def optimize_for_large_keysets(self, expected_keys: int = 1000):
        """
        Optimize the balancer for large key sets.
        
        Args:
            expected_keys: Expected number of keys for optimization
        """
        if expected_keys > 1000:
            # å¯¹äºå¤§é‡keyï¼Œè°ƒæ•´ç¼“å­˜ç­–ç•¥
            optimal_cache_size = min(expected_keys // 10, 1000)
            if optimal_cache_size != self.lru_cache.capacity:
                self.lru_cache = LRUCache(optimal_cache_size)
                print(f"ğŸ”„ Optimized cache size to {optimal_cache_size} for {expected_keys} keys")
            
            # è°ƒæ•´é€‰æ‹©é—´éš”
            self.min_selection_interval = max(0.01, 0.1 / (expected_keys / 100))
            print(f"ğŸ”„ Adjusted selection interval to {self.min_selection_interval:.3f}s")
    
    def batch_get_keys(self, batch_sizes: List[int]) -> List[List[str]]:
        """
        Get multiple batches of keys efficiently.
        
        Args:
            batch_sizes: List of batch sizes to retrieve
            
        Returns:
            List of key batches
        """
        total_requested = sum(batch_sizes)
        
        # æ›´æ–°æƒé‡åˆ†å¸ƒ
        self._update_weight_distribution()
        
        if total_requested > len(self._available_keys_list):
            raise RuntimeError(f"Requested {total_requested} keys but only {len(self._available_keys_list)} available")
        
        # ä¸€æ¬¡æ€§è·å–æ‰€æœ‰éœ€è¦çš„keys
        all_keys = self.get_keys(total_requested)
        
        # åˆ†å‰²æˆæ‰¹æ¬¡
        batches = []
        start_idx = 0
        for batch_size in batch_sizes:
            end_idx = start_idx + batch_size
            batches.append(all_keys[start_idx:end_idx])
            start_idx = end_idx
        
        return batches
    
    def get_database_info(self) -> dict:
        """
        Get database-specific information.
        
        Returns:
            Dictionary containing database information
        """
        stats = self.key_manager.get_key_stats()
        return {
            'database_path': self.key_manager.db_path,
            'database_size_mb': stats.get('database_size_mb', 0),
            'total_keys_in_db': stats.get('total_keys', 0),
            'available_keys_in_db': stats.get('available_keys', 0),
            'average_weight': stats.get('average_weight', 0),
            'source_distribution': stats.get('source_distribution', {}),
        }
    

    


